Africa_Soil_Property-competition
================================

Advances in rapid, low cost analysis of soil samples using infrared spectroscopy, georeferencing of soil samples, and greater availability of earth remote sensing data provide new opportunities for predicting soil functional properties at unsampled locations. Soil functional properties are those properties related to a soilâ€™s capacity to support essential ecosystem services such as primary productivity, nutrient and water retention, and resistance to soil erosion. Digital mapping of soil functional properties, especially in data sparse regions such as Africa, is important for planning sustainable agricultural intensification and natural resources management.

For the competition, we were asked to submit the predicted value for each of the five target soil functional properties - Ca, pH, P, SOC, and Sand - using mid-infrared absorbance measurements and remote sensing spatial data. Kaggle scores a portion of the test set publicly during the competition period, and reserves a portion for private scoring at the end of the competition. We were allowed to submit up to 3 predictions per day during the competition period, and had to choose two predictions for the final private scoring. Submissions are evaluated using the mean columnwise root mean squared error.

I first attempted to predict the target variables with an ordinary least squares linear regression model. The benefits of this method are that it's quick, there are no parameters to tune, and one can predict multiple targets. However, the model predictions were bad -- the Kaggle public score was 1.05 (equivalent score to the all zeros benchmark) -- so I moved on to more complex models. I used two methods, Support Vector regression and Gradient Boosting regression. Both of these methods required me to predict the five targets individually. I extensively grid searched the input parameters for these models, optimizing the parameters for each of the five target values.

The trickiest part of this Kaggle challenge was that only 13% of the test data were used to calculate the public score. There were only 1157 samples in the training set and 727 samples in the test set (with only 94 being used to calculate the public score), so there was a very large risk of overfitting. Because of this overfitting risk, I generally ignored the public Kaggle scores when selecting the two models for private scoring. Instead, I submitted the two models with the best cross-validation scores.

This Ipython notebook contains all of the code I used for this Kaggle competition.